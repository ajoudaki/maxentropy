{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "import tqdm as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in tqdm.tqdm(iterator, desc=\"Training\", leave=False):\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    num_classes = dict()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in tqdm.tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            top_pred = y_pred.argmax(1).cpu().data.numpy()\n",
    "            for c in top_pred:\n",
    "                if c not in num_classes:\n",
    "                    num_classes[c] = 1\n",
    "                else:\n",
    "                    num_classes[c] += 1\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), num_classes\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def plot_images(images):\n",
    "\n",
    "    n_images = len(images)\n",
    "\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(rows*cols):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(images[i].view(28, 28).cpu().numpy(), cmap='bone')\n",
    "        ax.axis('off')\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, D, L, res, activ, norm, D_in = None, D_out=None):\n",
    "        super().__init__()\n",
    "        assert(activ in ['lin', 'relu','tanh'])\n",
    "        assert(norm in ['', 'LN','BN1','BN2'])\n",
    "        if not D_in:\n",
    "            D_in = D\n",
    "        if not D_out:\n",
    "            D_out = D\n",
    "        self.norm = norm\n",
    "        self.activ = activ\n",
    "        self.fcs = [nn.Linear(D, D) for l in range(L)]\n",
    "        self.fcs[0] = nn.Linear(D_in,D)\n",
    "        self.fcs[-1] = nn.Linear(D,D_out)\n",
    "        for fc in self.fcs:\n",
    "            shape = fc.weight.shape\n",
    "            weights = 1.0/np.sqrt(shape[1])* torch.normal(0, 1, size=shape)\n",
    "            fc.weight.data = weights\n",
    "        \n",
    "        for li,fc in enumerate(self.fcs):\n",
    "            self.add_module(\"fc_\"+str(li), fc)\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.res = res\n",
    "        \n",
    "    def activation(self, h):\n",
    "        if self.activ=='lin':\n",
    "            return h\n",
    "        elif self.activ=='relu':\n",
    "            return F.relu(h)\n",
    "        elif self.activ=='tanh':\n",
    "            return torch.tanh(h)\n",
    "    \n",
    "    def normalize(self, h):\n",
    "        if self.norm=='LN':\n",
    "            # h = h - h.mean(1,keepdim=True)\n",
    "            h = h / torch.norm(h,dim=1,keepdim=True)\n",
    "        if self.norm=='BN2':\n",
    "            h = h - h.mean(0,keepdim=True)\n",
    "            h = h / torch.norm(h,dim=0,keepdim=True)\n",
    "        if self.norm=='BN1':\n",
    "            h = h / torch.norm(h,dim=0,keepdim=True)\n",
    "        return h \n",
    "        \n",
    "    def layer_update(self, l,h):\n",
    "        h2 = h\n",
    "        if l>0:\n",
    "            h2 = self.normalize(h2)\n",
    "        h2 = self.activation(h2)\n",
    "        h2 = self.fcs[l](h2)\n",
    "        if l==self.L-1:\n",
    "            h2 = torch.softmax(h2,1)\n",
    "        return h2\n",
    "\n",
    "\n",
    "    def full_forward(self, h):\n",
    "        h = h.view(h.shape[0],-1) # flatten images to vectors\n",
    "        H = [h.cpu().data.numpy()]\n",
    "        for l in range(self.L):\n",
    "            h = self.layer_update(l,h)\n",
    "            H.append(h.cpu().data.numpy())\n",
    "        return H\n",
    "    \n",
    "    def forward(self, h):\n",
    "        h = h.view(h.shape[0],-1) #  # flatten images to vectors\n",
    "        for l in range(self.L):\n",
    "            h = self.layer_update(l,h)\n",
    "        return h\n",
    "    \n",
    "def show_layers(Hidden, Num=None,subplot=True,title=False,save_path=None):\n",
    "    if not Num:\n",
    "        Num = len(Hidden)\n",
    "    inds = np.linspace(0,len(Hidden)-1,Num).astype(np.int32)\n",
    "    Hidden = [Hidden[i] for i in inds]\n",
    "    if subplot:\n",
    "        fig = plt.figure(figsize=(2*Num,2))\n",
    "    for Hi,(l,H) in enumerate(zip(inds,Hidden)):\n",
    "        while H.shape[0]==1:\n",
    "            H = H[0]\n",
    "        # H = H.data.numpy()\n",
    "        if subplot:\n",
    "            ax = fig.add_subplot(1,Num,Hi+1)\n",
    "        else:\n",
    "            plt.clf()\n",
    "            ax = plt.gca()\n",
    "        \n",
    "        ax.scatter(H[0],H[1],2)\n",
    "        if title:\n",
    "            ax.set_title(f'Layer = {Hi+1}')\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f6e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '.data'\n",
    "train_data = datasets.MNIST(root=ROOT,\n",
    "                            train=True,\n",
    "                            download=True)\n",
    "\n",
    "mean = train_data.data.float().mean() / 255\n",
    "std = train_data.data.float().std() / 255\n",
    "print(f'Calculated mean: {mean}')\n",
    "print(f'Calculated std: {std}')\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                            transforms.RandomRotation(5, fill=(0,)),\n",
    "                            transforms.RandomCrop(28, padding=2),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[mean], std=[std])\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[mean], std=[std])\n",
    "                                     ])\n",
    "\n",
    "train_data = datasets.MNIST(root=ROOT,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=train_transforms)\n",
    "\n",
    "test_data = datasets.MNIST(root=ROOT,\n",
    "                           train=False,\n",
    "                           download=True,\n",
    "                           transform=test_transforms)\n",
    "\n",
    "Classes = list(range(10))\n",
    "Classes = [0,1]\n",
    "train_data = [(x,y) for x,y in train_data if y in Classes]\n",
    "test_data = [(x,y) for x,y in test_data if y in Classes]\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "N_IMAGES = 25\n",
    "\n",
    "images = [image for image, label in [test_data[i] for i in range(N_IMAGES)]]\n",
    "\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3896b9-3dc8-4438-8690-b46feb014f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([image for image, label in train_data if label<=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2ef65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def class_freq(num_classes,classes, prob=True, label = ''):\n",
    "    freq = np.zeros(len(classes))\n",
    "    for c, n in num_classes.items():\n",
    "        freq[c] = n\n",
    "    if prob:\n",
    "        freq = freq / np.sum(freq)\n",
    "    s = ', '.join([f'f[{c}]={f:.3f}' for f in freq])\n",
    "    s = label + ': ' + s\n",
    "    print(s)\n",
    "    return s\n",
    "def calc_H(num_classes, classes):\n",
    "    freq = np.zeros(len(classes))\n",
    "    for c, n in num_classes.items():\n",
    "        freq[c] = n\n",
    "    freq = freq + 1e-2/sum(freq) # to avoid NaN in log\n",
    "    freq = freq / np.sum(freq)\n",
    "    tv = np.mean(abs(freq - 1.0/len(freq)))\n",
    "    return -np.sum(freq * np.log2(freq)),tv\n",
    "\n",
    "repeat = 50\n",
    "EPOCHS = 10\n",
    "\n",
    "D_IN = 28*28\n",
    "num_classes = len(Classes)\n",
    "WIDTH = 200\n",
    "LAYERS = 15\n",
    "\n",
    "RES = .0\n",
    "BATCH_SIZE = 16\n",
    "ACTIVATION = 'relu'\n",
    "NORM = ''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df = pd.DataFrame(columns=['epoch','bath_size','width','layers','normalization','activation', 'H_train','H_val', 'TV_train','TV_test', 'train_acc','val_acc','train_loss','val_loss',])\n",
    "\n",
    "\n",
    "for ri in range(repeat):\n",
    "    print(ri, repeat)\n",
    "    for NORM in ['','BN2']:\n",
    "        train_iterator = data.DataLoader(train_data,batch_size=BATCH_SIZE)\n",
    "        test_iterator = data.DataLoader(test_data,batch_size=BATCH_SIZE)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        model = MyMLP(L=LAYERS,D=WIDTH,res=RES,activ=ACTIVATION, norm=NORM, D_in=D_IN, D_out=num_classes)\n",
    "        model = model.to(device)\n",
    "\n",
    "        best_valid_loss = float('inf')\n",
    "        optimizer = optim.SGD(model.parameters(),lr=1e-3)\n",
    "\n",
    "\n",
    "        for epoch in range(EPOCHS+1):\n",
    "            if epoch>0:\n",
    "                train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "            else:\n",
    "                train_loss, train_acc = None, None\n",
    "            valid_loss, valid_acc, freq = evaluate(model, test_iterator, criterion, device)\n",
    "            class_freq(freq, Classes, 'Valid. freqs')\n",
    "            # _, _, num_classes = evaluate(model, train_iterator, criterion, device)\n",
    "            _, _, train_num_classes = evaluate(model, train_iterator, criterion, device)\n",
    "            _, _, test_num_classes = evaluate(model, test_iterator, criterion, device)\n",
    "            H_train,TV_train = calc_H(train_num_classes, Classes)\n",
    "            H_test,TV_test = calc_H(test_num_classes, Classes)\n",
    "            df.loc[len(df),:] = (epoch, BATCH_SIZE,WIDTH,LAYERS, NORM,ACTIVATION,H_train,TV_test, TV_train,TV_test, train_acc, valid_acc, train_loss, valid_loss)\n",
    "            if epoch==0:\n",
    "                continue\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "            # print(f'Epoch: {epoch+1}\\tTrain Loss: {train_loss:.5f}\\tTrain Acc: {train_acc}\\tVal. Loss: {valid_loss:.5f}\\t Val. Acc: {valid_acc:.5f}')\n",
    "            # sample_plot(10,2)\n",
    "            # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e7f9d-1dd1-4d36-99b5-0c86050f5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "df.normalization[df.normalization==''] = 'Vanilla'\n",
    "df.normalization[df.normalization=='BN2'] = 'BN'\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.lineplot(ax=axes[0], data=df, x='epoch',y='H_train',hue='normalization',marker='o')\n",
    "axes[0].set_ylabel('Model entropy (bits)')\n",
    "axes[0].set_title('(A) Model entropy vs. epoch',fontsize=12)\n",
    "sns.lineplot(ax=axes[1], data=df, x='epoch',y='train_acc',hue='normalization',marker='o')\n",
    "axes[1].set_title('(B) Loss vs. epoch',fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "\n",
    "\n",
    "fig.savefig('entropy_vs_SGD.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020592c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "WIDTH = 100\n",
    "LAYERS = 50\n",
    "\n",
    "RES = .0\n",
    "repeat = 10\n",
    "BATCH_SIZE = 16\n",
    "ACTIVATION = 'lin'\n",
    "NORM = ''\n",
    "df2 = pd.DataFrame(columns=['bath_size','width','layers','normalization','activation', 'H_train','H_test', 'TV_train','TV_test'])\n",
    "\n",
    "for ri in range(repeat):\n",
    "    print(ri, repeat)\n",
    "    for BATCH_SIZE in [16]:\n",
    "        for WIDTH in [50,100]:\n",
    "            for LAYERS in [2,5,10]:\n",
    "                for ACTIVATION in ['relu']:#['lin','tanh','relu']:\n",
    "                    for NORM in ['','BN2']:\n",
    "                        train_iterator = data.DataLoader(train_data,batch_size=BATCH_SIZE)\n",
    "                        test_iterator = data.DataLoader(test_data,batch_size=BATCH_SIZE)\n",
    "\n",
    "                        model = MyMLP(L=LAYERS,D=WIDTH,res=RES,activ=ACTIVATION, norm=NORM, D_in=D_IN, D_out=num_classes)\n",
    "                        model = model.to(device)\n",
    "\n",
    "                        valid_loss, valid_acc, train_num_classes = evaluate(model, train_iterator, criterion, device)\n",
    "                        valid_loss, valid_acc, test_num_classes = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "                        # print(f'{train_num_classes}, {test_num_classes}')\n",
    "                        H_train,TV_train = calc_H(train_num_classes, Classes)\n",
    "                        H_test,TV_test = calc_H(test_num_classes, Classes)\n",
    "                        df2.loc[len(df2),:] = (BATCH_SIZE,WIDTH,LAYERS, NORM,ACTIVATION,H_train,H_test, TV_train,TV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388ad2b-a436-475c-91e2-e618c39be510",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "df2.normalization[df2.normalization==''] = 'Vanilla'\n",
    "df2.normalization[df2.normalization=='BN2'] = 'BN'\n",
    "df2.activation[df2.activation=='lin'] = 'linear'\n",
    "\n",
    "sub = df2.loc[(df2.activation=='relu')].rename(columns={'H_train': 'Model entropy'})\n",
    "# sns.lineplot(data=sub,x='layers',y='Model entropy',hue='normalization',style='width')\n",
    "# plt.savefig('entropy_vs_depth.pdf')\n",
    "\n",
    "# g = sns.FacetGrid(sub, col=\"width\", hue=\"normalization\")\n",
    "# g.map(sns.lineplot, \"layers\", \"Model entropy\", markers=True)\n",
    "# g.add_legend()\n",
    "# plt.savefig('entropy_vs_depth.pdf')\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(6*2,5))\n",
    "L = 'ABC'\n",
    "for axi, (ax, w) in enumerate(zip(axes, sorted(df2.width.unique()))):\n",
    "    sns.lineplot(ax=ax, data=sub.loc[sub.width==w],x='layers',y='Model entropy',hue='normalization',marker='o')\n",
    "    ax.set_xlabel('depth')\n",
    "    if axi==0:\n",
    "        ax.set_ylabel('Model entropy (bits)')\n",
    "    else:\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_title(f'({L[axi]}) $width = {w}$',fontsize=15)\n",
    "    ax.set_ylim(0,1)\n",
    "    # plt.xscale('log')\n",
    "fig.savefig('entropy_vs_depth.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fc09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
